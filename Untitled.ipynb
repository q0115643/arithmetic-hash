{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RNN Model and Check Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "model_path = './data/checkpoint/rnn3.pkl'\n",
    "iter_print_cycle = 50\n",
    "state = torch.load(model_path)\n",
    "eval_loss_record = state['eval_loss_record']\n",
    "train_loss_record = state['train_loss_record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Validation Loss in Gradient Descent\")\n",
    "eval_loss_xrange = [(i+1)*iter_print_cycle for i in list(range(len(eval_loss_record)))]\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(eval_loss_xrange, eval_loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Training Batch Loss in Gradient Descent\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "from models import CharLSTM, Markov\n",
    "from torch.autograd import Variable\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import string\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(message)s', datefmt='%m-%d %H:%M', stream=sys.stdout)\n",
    "using_GPU = torch.cuda.is_available()\n",
    "train_tokens_fp = './data/train_tokens.txt'\n",
    "test_tokens_fp = './data/test_tokens.txt'\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_probs = []\n",
    "first_char_test = []\n",
    "with open(test_tokens_fp) as f:\n",
    "    test_tokens = f.readlines()\n",
    "    test_tokens = [x.strip() for x in test_tokens] \n",
    "    for tok in test_tokens:\n",
    "        first_char_test.append(tok[0])\n",
    "first_char_test = sorted(Counter(first_char_test).items(), key=lambda pair: pair[0], reverse=False)\n",
    "total_cnt = 0\n",
    "for (_, cnt) in first_char_test:\n",
    "    total_cnt += cnt\n",
    "char_probs.append(tuple([\"<END>\", 0]))\n",
    "for idx, (c, cnt) in enumerate(first_char_test):\n",
    "    char_probs.append(tuple([c, float(cnt)/float(total_cnt)]))\n",
    "print(char_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = list(string.ascii_lowercase)\n",
    "alphabet_size = len(alphabets) + 2\n",
    "int2char = dict(enumerate(alphabets, start=2))\n",
    "int2char[0] = '<PAD>'\n",
    "int2char[1] = '<END>'\n",
    "RNN_model = CharLSTM(alphabet_size=alphabet_size,\n",
    "                     hidden_dim=hidden_dim)\n",
    "if using_GPU:\n",
    "    RNN_model = RNN_model.cuda()\n",
    "state = torch.load(model_path)\n",
    "RNN_model.load_state_dict(state['model'])\n",
    "RNN_model.eval()\n",
    "hidden = RNN_model.init_hidden()\n",
    "start = [torch.Tensor(np.zeros((1, alphabet_size)))]\n",
    "start = torch.stack(start)\n",
    "with torch.no_grad():\n",
    "    start = Variable(start)\n",
    "    if using_GPU:\n",
    "        start = start.cuda()\n",
    "        hidden = (hidden[0].cuda(), hidden[1].cuda())\n",
    "    output, hidden = RNN_model.forward2(start, hidden)\n",
    "predicted_char_probs = []\n",
    "pad_prob = 0\n",
    "for idx, prob in enumerate(list(output.cpu().numpy()[0][0])):\n",
    "    if idx == 1:\n",
    "        prob += pad_prob\n",
    "    if idx != 0:\n",
    "        predicted_char_probs.append((int2char[idx], prob))\n",
    "    else:\n",
    "        pad_prob = prob\n",
    "print(predicted_char_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0\n",
    "logging.info(\"Comparing Actual Probabilities of the First Character with Predicted Probabilities by LSTM\")\n",
    "for actual, predicted in zip(char_probs, predicted_char_probs):\n",
    "    difference = predicted[1] - actual[1]\n",
    "    std_dev += difference * difference\n",
    "    print('char: {} \\tactual: {:.5f}  \\tpredicted: {:.5f} \\tdifference: {:.5f}'.format(actual[0],\n",
    "                                                                                       actual[1],\n",
    "                                                                                       predicted[1],\n",
    "                                                                                       difference))\n",
    "import math\n",
    "std_dev = math.sqrt(std_dev)\n",
    "logging.info('Standard Deviation: {:.5f}'.format(std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Markov\n",
    "\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "alphabet_size = len(alphabets) + 1\n",
    "int2char = dict(enumerate(alphabets, start=1))\n",
    "int2char[0] = '<PAD>'\n",
    "char2int = {char: index for index, char in int2char.items()}\n",
    "\n",
    "with open(train_tokens_fp) as f:\n",
    "    train_tokens = f.readlines()\n",
    "    train_tokens = [x.strip() for x in train_tokens] \n",
    "mc = Markov(3)\n",
    "mc.learn(train_tokens)\n",
    "markov_char_probs = mc.get_probs('')\n",
    "print(markov_char_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_probs = []\n",
    "first_char_test = []\n",
    "with open(test_tokens_fp) as f:\n",
    "    test_tokens = f.readlines()\n",
    "    test_tokens = [x.strip() for x in test_tokens] \n",
    "    for tok in test_tokens:\n",
    "        first_char_test.append(tok[0])\n",
    "first_char_test = sorted(Counter(first_char_test).items(), key=lambda pair: pair[0], reverse=False)\n",
    "total_cnt = 0\n",
    "for (_, cnt) in first_char_test:\n",
    "    total_cnt += cnt\n",
    "char_probs.append(tuple([\"<END>\", 0]))\n",
    "for idx, (c, cnt) in enumerate(first_char_test):\n",
    "    char_probs.append(tuple([c, (1 + float(cnt))/(float(total_cnt) + alphabet_size)]))\n",
    "\n",
    "std_dev = 0\n",
    "logging.info(\"Comparing Actual Probabilities of the First Character with Predicted Probabilities by Markov Chain\")\n",
    "for actual, predicted in zip(char_probs, markov_char_probs):\n",
    "    difference = predicted[1] - actual[1]\n",
    "    std_dev += difference * difference\n",
    "    print('char: {} \\tactual: {:.5f}  \\tpredicted: {:.5f} \\tdifference: {:.5f}'.format(actual[0],\n",
    "                                                                                       actual[1],\n",
    "                                                                                       predicted[1],\n",
    "                                                                                       difference))\n",
    "import math\n",
    "std_dev = math.sqrt(std_dev)\n",
    "logging.info('Standard Deviation: {:.5f}'.format(std_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token -> Position from [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import RNN_Map, Markov_Map\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "using_GPU = torch.cuda.is_available()\n",
    "num_node = 500\n",
    "rnn_map = RNN_Map(model_path, num_node, using_GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_map.get_node('south'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_fp = './data/train_tokens.txt'\n",
    "markov_map = Markov_Map(3, train_tokens_fp, num_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(markov_map.get_node('south'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens_fp = './data/test_tokens.txt'\n",
    "token_counter = []\n",
    "with open(test_tokens_fp) as f:\n",
    "    test_tokens = f.readlines()\n",
    "    test_tokens = [x.strip() for x in test_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_per_node = [0] * num_node\n",
    "for token in tqdm(test_tokens):\n",
    "    node_num = markov_map.get_node(token)\n",
    "    cnt_per_node[node_num - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Zipf distirbution from Markov Chain based Mapper\")\n",
    "zipf_xrange = [i+1 for i in list(range(500))]\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(top=600)\n",
    "plt.plot(zipf_xrange, cnt_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_node = len(test_tokens)/float(num_node)\n",
    "variance = 0\n",
    "for cnt in cnt_per_node:\n",
    "    diff = cnt - avg_node\n",
    "    variance += diff*diff\n",
    "std_dev = math.sqrt(variance/len(test_tokens))\n",
    "print(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_per_node = [0] * num_node\n",
    "for token in tqdm(test_tokens):\n",
    "    node_num = rnn_map.get_node(token)\n",
    "    cnt_per_node[node_num - 1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Zipf distirbution from RNN based Mapper\")\n",
    "zipf_xrange = [i+1 for i in list(range(num_node))]\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(top=600)\n",
    "plt.plot(zipf_xrange, cnt_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_node = len(test_tokens)/float(num_node)\n",
    "variance = 0\n",
    "for cnt in cnt_per_node:\n",
    "    diff = cnt - avg_node\n",
    "    variance += diff*diff\n",
    "std_dev = math.sqrt(variance/len(test_tokens))\n",
    "print(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (other-env)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
